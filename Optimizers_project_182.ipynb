{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO6VVlYbXj2e9xuejx02f4x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fantiflex/MuOn-optimizer/blob/main/Optimizers_project_182.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**\n",
        "\n"
      ],
      "metadata": {
        "id": "ZFsIyq9fH8df"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pxUtCGhTH4VC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**L-BFGS optimizer class setup**"
      ],
      "metadata": {
        "id": "ymcDnaqFIbbf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ManifoldLBFGS:\n",
        "  \"\"\"\n",
        "    Riemannian L-BFGS sur Stiefel avec:\n",
        "      - métrique euclidienne (produit de Frobenius)\n",
        "      - rétraction polaire (msign/polar)\n",
        "      - transport par projection\n",
        "    Utilisation typique:\n",
        "        opt = ManifoldLBFGS(eta=0.1, history=10)\n",
        "        # itération k\n",
        "        W = opt.step(W, Gk)           # prend un pas, renvoie W_{k+1}\n",
        "        ...\n",
        "        Gkp1 = ...                    # gradient au nouveau point (fourni par toi)\n",
        "        opt.update(Gkp1)              # met à jour l'historique (s_k, y_k)\n",
        "    \"\"\"\n",
        "  def __init__(self, eta=0.1, history=10, eps_curv=1e-12, use_polar_impl=True):\n",
        "    '''\n",
        "    eta is the step size for the primal problem\n",
        "    history is the memory length for L-BGFS\n",
        "    eps_curv avoids unfortunate updates\n",
        "    '''\n",
        "    self.eta = eta\n",
        "    self.m = history\n",
        "    self.eps_curv = eps_curv\n",
        "    self.S, self.Y, self.RHO = [], [], []\n",
        "    self.last = None\n",
        "    self.use_polar_impl = use_polar_impl\n",
        "\n",
        "  # _retract ensures that W stays a Stiefel\n",
        "  def _retract(self, X):\n",
        "    return polar_retraction(X) if self.use_polar_impl else msign(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def two_loops(self, q):\n",
        "    '''\n",
        "    L-BFGS two-loop recursion to apply the approximate inverse Hessian H_k to vector q (current gradient already projected to the tangent).\n",
        "    Returns H_k q (descent direction).\n",
        "    Uses stored (s_i, y_i) with ρ_i = 1/<s_i, y_i>.\n",
        "    '''\n",
        "    if len(self.S) == 0: # No curvature info yet -> fallback to steepest descent\n",
        "          return -q\n",
        "    else :\n",
        "      alpha = []\n",
        "      for s, y, rho in reversed(list(zip(self.S, self.Y, self.RHO))):\n",
        "        # ===== Backward loop=====\n",
        "        # Remove components of q along recent curvature directions y_i.\n",
        "        # This emulates multiplying by the right-hand factors in the (inverse) BFGS formula,\n",
        "        # while only storing vectors. Each step uses a scalar α_i = ρ_i <s_i, q>.\n",
        "        a = rho * frob_inner(s, q)   # α_i = ρ_i * <s_i, q>   (scalar)\n",
        "        alpha.append(a)\n",
        "        q = q - a * y                # q ← q - α_i y_i  (matrix update)\n",
        "\n",
        "      # ===== Initial scaling: H0 = γ I =====\n",
        "      # γ scales the identity so that H0 y_last ≈ s_last, i.e., matches recent local curvature.\n",
        "      y_last = self.Y[-1]\n",
        "      s_last = self.S[-1]\n",
        "      sy = frob_inner(s_last, y_last)  # <s_{m-1}, y_{m-1}>  (positive by safeguard)\n",
        "      yy = frob_inner(y_last, y_last)  # <y_{m-1}, y_{m-1}>\n",
        "      gamma = (sy / yy) if yy > 0 else 1.0  # robust fallback if yy ≈ 0\n",
        "      r = gamma * q                    # r ≈ H0 q\n",
        "\n",
        "      for (s, y, rho), a in zip(zip(self.S, self.Y, self.RHO), reversed(alpha)):\n",
        "        # ===== Forward loop=====\n",
        "        # Rebuild the action of H_k using s_i and the stored α_i.\n",
        "        # β_i = ρ_i <y_i, r>, then r ← r + s_i (α_i - β_i).\n",
        "        # This sequence applies the low-rank BFGS corrections in the correct order.\n",
        "        beta = rho * frob_inner(y, r)  # β_i = ρ_i * <y_i, r>  (scalar)\n",
        "        r = r + s * (a - beta)         # r ← r + s_i (α_i - β_i) (matrix)\n",
        "\n",
        "      #r ≈ H_k g_k the final descent direction\n",
        "      return r\n",
        "\n",
        "\n",
        "  def step(self, W, G):\n",
        "    \"\"\"\n",
        "      Take a single quasi-Newton step from W using raw gradient G (same shape as W).\n",
        "      Pipeline:\n",
        "        1) If W is wide (n < p), transpose to operate in tall shape for Stiefel stability.\n",
        "        2) Project raw gradient onto tangent space: g = Proj_TW(G).\n",
        "        3) Build quasi-Newton direction d = -H_k g via two-loop recursion.\n",
        "        4) Extra projection for numerical safety: d = Proj_TW(d).\n",
        "        5) Take ambient step and retract: W_new = Retr(W - eta * d).\n",
        "      Returns:\n",
        "          W_new with the same shape/orientation as the input W (orthonormal columns).\n",
        "      \"\"\"\n",
        "    # Ensure W and G have the same shape\n",
        "    assert W.shape == G.shape, f\"W and G must have the same shape, but got W.shape={W.shape} and G.shape={G.shape}\"\n",
        "\n",
        "    should_transpose = W.shape[0] < W.shape[1]\n",
        "    # The print statements below are for debugging and can be removed in a production environment.\n",
        "    # print(f\"DEBUG (ManifoldLBFGS.update): should_transpose: {should_transpose}\")\n",
        "    # print(f\"DEBUG (ManifoldLBFGS.update): W_new shape from last step: {W.shape}\")\n",
        "    # print(f\"DEBUG (ManifoldLBFGS.update): G_new shape (raw from p.grad) before transpose logic: {G.shape}\")\n",
        "    if should_transpose:\n",
        "        W = W.T\n",
        "        G = G.T\n",
        "    # print(f\"DEBUG (ManifoldLBFGS.update): Calling tangent_proj with W_new shape: {W.shape}, G_new shape: {G.shape}\")\n",
        "    # Riemannian gradient (tangent at W)\n",
        "    #print('1 tour step')\n",
        "    g = tangent_proj(W, G)\n",
        "\n",
        "    # Quasi-Newton direction via L-BFGS memory\n",
        "    d = self.two_loops(g)\n",
        "\n",
        "    # Re-project in case of small numerical drift\n",
        "    d = tangent_proj(W, d)\n",
        "\n",
        "    # Ambient move + retraction back to the Stiefel\n",
        "    W_new = W - self.eta * d\n",
        "    W_new = self._retract(W_new)\n",
        "\n",
        "    # Cache objects needed to form (s_k, y_k) once G_new is available\n",
        "    self.last = {\n",
        "        \"W\": W,\n",
        "        \"W_new\": W_new,\n",
        "        \"g\": g,\n",
        "        \"d\": d,\n",
        "        \"should_transpose\": should_transpose\n",
        "    }\n",
        "    return W_new.T if should_transpose else W_new\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def update(self, G_new):\n",
        "      \"\"\"\n",
        "      Update the L-BFGS history once the new gradient at W_new is known.\n",
        "      Pipeline:\n",
        "        1) Read cache from the last call to step().\n",
        "        2) Build new Riemannian gradient: g_new = Proj_{T_{W_new}}(G_new).\n",
        "        3) Transport previous step and gradient to the new tangent space:\n",
        "              s_k = Transport(eta * d)           ∈ T_{W_new}\n",
        "              y_k = g_new - Transport(g)          ∈ T_{W_new}\n",
        "        4) Curvature test: require <s_k, y_k> > eps_curv to keep H_k PD.\n",
        "        5) Push (s_k, y_k, 1/<s_k, y_k>) into rolling buffers; drop oldest if needed.\n",
        "      Returns:\n",
        "          True if the pair was accepted; False if rejected by curvature safeguard.\n",
        "      \"\"\"\n",
        "      assert self.last is not None\n",
        "\n",
        "      W = self.last[\"W\"]\n",
        "\n",
        "\n",
        "\n",
        "      # Ensure W and G have the same shape\n",
        "      #print('1 début de update')\n",
        "\n",
        "      W_new  = self.last[\"W_new\"]\n",
        "      d      = self.last[\"d\"]\n",
        "      g      = self.last[\"g\"]\n",
        "      should_transpose = self.last[\"should_transpose\"]\n",
        "      #print(f\"DEBUG (ManifoldLBFGS.update): should_transpose: {should_transpose}\")\n",
        "      #print(f\"DEBUG (ManifoldLBFGS.update): W_new shape from last step: {W_new.shape}\")\n",
        "      #print(f\"DEBUG (ManifoldLBFGS.update): G_new shape (raw from p.grad) before transpose logic: {G_new.shape}\")\n",
        "\n",
        "      if should_transpose:\n",
        "          G_new = G_new.T\n",
        "\n",
        "      # print(f\"DEBUG (ManifoldLBFGS.update): Calling tangent_proj with W_new shape: {W_new.shape}, G_new shape: {G.shape}\")\n",
        "      #assert W.shape == G_new.shape, f\"W and G must have the same shape, but got W.shape={W.shape} and G.shape={G_new.shape}\"\n",
        "      # New Riemannian gradient\n",
        "      g_new = tangent_proj(W_new, G_new)\n",
        "\n",
        "      # Form (s_k, y_k) in the new tangent space\n",
        "      s = transport_by_projection(W, W_new, -self.eta * d)   # displacement\n",
        "      y = g_new - transport_by_projection(W, W_new, g)      # grad change\n",
        "\n",
        "\n",
        "      # Curvature condition <s, y> > 0 for positive definite inverse Hessian\n",
        "      sy = frob_inner(s, y)\n",
        "      if not torch.isfinite(sy) or sy <= self.eps_curv:\n",
        "          # Reject bad curvature to keep the inverse Hessian approximation well-conditioned\n",
        "          self.last = None\n",
        "          return False\n",
        "\n",
        "      # Maintain limited memory (FIFO)\n",
        "      if len(self.S) == self.m:\n",
        "          self.S.pop(0); self.Y.pop(0); self.RHO.pop(0)\n",
        "      self.S.append(s.detach().clone())\n",
        "      self.Y.append(y.detach().clone())\n",
        "      self.RHO.append(1.0 / sy)\n",
        "\n",
        "      self.last = None\n",
        "      return True"
      ],
      "metadata": {
        "id": "QqrjwWZvIa-X"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Useful geometrical riemaniann functions**"
      ],
      "metadata": {
        "id": "OKuMrrQqVhTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _sym(X):\n",
        "    \"\"\"Return the symmetric part of a square matrix: (X + X^T)/2.\"\"\"\n",
        "    return 0.5 * (X + X.T)\n",
        "\n",
        "def tangent_proj(W, Z):\n",
        "  \"\"\"\n",
        "  Project an ambient matrix Z onto the tangent space of the Stiefel manifold at W.\n",
        "  Tangent space condition at W: W^T Δ + Δ^T W = 0  (skew-symmetry).\n",
        "  Projection formula: Proj(Z) = Z - W * Sym(W^T Z).\n",
        "  Shapes: W ∈ R^{n×p} with orthonormal columns; Z ∈ R^{n×p}; return ∈ R^{n×p}.\n",
        "  \"\"\"\n",
        "  n, p = W.shape\n",
        "  if n >= p:\n",
        "      # tall : formule standard\n",
        "      #print(f\"DEBUG: In tangent_proj (n >= p branch):\")\n",
        "      #print(f\"DEBUG: W shape: {W.shape}\")\n",
        "      #print(f\"DEBUG: Z shape: {Z.shape}\")\n",
        "      return Z - W @ (0.5 * ((W.T @ Z) + (W.T @ Z).T))\n",
        "  else:\n",
        "      # wide : travaille dans l'espace transposé (tall), puis retranspose\n",
        "      Wt, Zt = W.T, Z.T            # shapes: p x n\n",
        "      Pt = Zt - Wt @ (0.5 * ((Wt.T @ Zt) + (Wt.T @ Zt).T))  # p x n\n",
        "      return Pt.T                   # n x p\n",
        "\n",
        "\n",
        "def polar_retraction(X):\n",
        "    \"\"\"\n",
        "    Retract an ambient matrix back to the Stiefel manifold using the polar factor.\n",
        "    This returns the matrix with orthonormal columns closest to X in Frobenius norm.\n",
        "    Prefer torch.linalg.polar when available; fall back to SVD otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # torch >= 2.1: exact polar factorization, numerically stable\n",
        "        U, _ = torch.linalg.polar(X)\n",
        "    except Exception:\n",
        "        # Fallback: X = U Σ V^T  => polar(X) = U V^T\n",
        "        U, _, Vt = torch.linalg.svd(X, full_matrices=False)\n",
        "        U = U @ Vt\n",
        "    return U\n",
        "\n",
        "def transport_by_projection(W_old, W_new, Xi):\n",
        "    \"\"\"\n",
        "    Vector transport from T_{W_old} to T_{W_new} by simple re-projection.\n",
        "    This is NOT isometric but is widely used in practice: T(Xi) = Proj_{T_{W_new}}(Xi).\n",
        "    Keeps code simple, robust, and compatible with polar retraction.\n",
        "    \"\"\"\n",
        "    return tangent_proj(W_new, Xi)\n",
        "\n",
        "def frob_inner(X, Y):\n",
        "    \"\"\"\n",
        "    Frobenius inner product <X, Y> = trace(X^T Y). Works for same-shaped matrices.\n",
        "    Used to compute curvature scalars (sᵀy), scaling, and two-loop recursion scalars.\n",
        "    \"\"\"\n",
        "    return torch.tensordot(X, Y, dims=([0,1],[0,1]))"
      ],
      "metadata": {
        "id": "Q9je4DdNYr7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Original version from Jeremy Bernstein's paper**"
      ],
      "metadata": {
        "id": "rO2NxORbbwWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad() #context manager in PyTorch that disables gradient calculation within its scope. This is particularly useful during inference, validation, or when performing operations where you do not intend to update model parameters and therefore do not need to compute gradients.\n",
        "\n",
        "def manifold_muon(W, G, eta=0.1, alpha=0.01, steps=100, tol=1e-6):\n",
        "  '''\n",
        "  W is a 2D pytorch matric, current weights\n",
        "  G is a 2D pytorch matric, loss gradient\n",
        "  eta is the step size for the primal problem\n",
        "  alpha is the step size for the dual problem\n",
        "  steps is the number of steps for the dual problem\n",
        "  tol is the tolerance for the stopping criterion\n",
        "  '''\n",
        "  # Ensure that W and G are both tall matrices (more stable for Stiefel)\n",
        "  should_tranpose = W.shape[0] < W.shape[1]\n",
        "  if should_tranpose:\n",
        "      W = W.T\n",
        "      G = G.T\n",
        "\n",
        "\n",
        "  # Initialize the dual variable\n",
        "  Lambda = -0.25 * (W.T @ G + G.T @ W)\n",
        "\n",
        "\n",
        "  # Ascend on the dual problem to find the update direction A\n",
        "  for step in range(steps):\n",
        "      # Update the candidate direction A\n",
        "      A = msign(G + 2 * W @ Lambda)\n",
        "      # Measure deviation of A from the tangent space:\n",
        "      H = W.T @ A + A.T @ W\n",
        "      # Check the stopping criterion\n",
        "      if torch.norm(H) / math.sqrt(H.numel()) < tol:\n",
        "          break\n",
        "      # Update the dual variable\n",
        "      Lambda -= alpha * (1 - step / steps) * H\n",
        "  # Descend on the primal problem\n",
        "  new_W = W - eta * A\n",
        "  # Retract to the manifold\n",
        "  new_W = msign(new_W)\n",
        "  # Restore the shape of the solution and return\n",
        "  return new_W.T if should_tranpose else new_W"
      ],
      "metadata": {
        "id": "S8brQ45AH6S4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Manifold MuOn adaptable for other optimizer instances**"
      ],
      "metadata": {
        "id": "l5hdOFPxcRIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def manifold_muon_general( W, G, eta=0.1, alpha=0.01, steps=100, tol=1e-6, *, opt = ManifoldLBFGS(eta=0.1, history=10)):\n",
        "    \"\"\"\n",
        "    Drop-in wrapper that replaces the Muon direction with a Riemannian L-BFGS step.\n",
        "\n",
        "    Args:\n",
        "        W (torch.Tensor): current point (n x p), ideally on Stiefel (W^T W ≈ I).\n",
        "        G (torch.Tensor): raw gradient d(loss)/dW, same shape as W.\n",
        "        eta (float): step size; overrides opt.eta for this call.\n",
        "        alpha, steps, tol: kept for signature compatibility (ignored here).\n",
        "        opt (ManifoldLBFGS): persistent optimizer instance.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: new W on the Stiefel manifold (same shape/orientation as input).\n",
        "\n",
        "    Notes:\n",
        "        - This function ONLY performs the STEP. After you recompute the gradient\n",
        "          at the returned W, call opt.update(G_new) once to feed (s_k, y_k) to L-BFGS.\n",
        "        - Removed redundant tall/wide handling from this wrapper; ManifoldLBFGS.step()\n",
        "          now handles it internally and returns W in the original orientation.\n",
        "    \"\"\"\n",
        "    assert opt is not None, \"Pass your ManifoldLBFGS instance via opt=...\"\n",
        "    # Ensure W and G have the same shape\n",
        "    assert W.shape == G.shape, f\"W and G must have the same shape, but got W.shape={W.shape} and G.shape={G.shape}\"\n",
        "\n",
        "    # Set per-step step size (if you want to schedule eta externally, set opt.eta there)\n",
        "    opt.eta = eta\n",
        "\n",
        "    # Take one quasi-Newton step on the manifold (retraction included inside .step)\n",
        "    # ManifoldLBFGS.step handles internal transposition and returns W in the original orientation.\n",
        "    new_W = opt.step(W, G)\n",
        "\n",
        "    # The new_W returned by opt.step is already in the original orientation.\n",
        "    return new_W"
      ],
      "metadata": {
        "id": "bAv6AfqFcgCB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}