{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOtplAVGKO8NqwBjiW63c3t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fantiflex/MuOn-optimizer/blob/main/MainV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"gpu\")\n",
        "\n",
        "%run \"/content/drive/MyDrive/Colab_Notebooks/EECS182_project/hyperspherical_descent.ipynb\"\n",
        "%run \"/content/drive/MyDrive/Colab_Notebooks/EECS182_project/Optimizers_project_182.ipynb\"\n",
        "# after this, the functions defined inside those notebooks are available in the current notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX_PDnXkgWkM",
        "outputId": "25f1ed3e-f3fd-4dd1-bec3-a23741879344"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "em6Mx4-5gGZM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22960955-16dc-440a-93ce-6a2a709f69c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with: manifold_muon_general\n",
            "Epochs: 5 --- LR: 0.1 \n",
            "Epoch 1, Loss: 3.9194172596444887, Time: 12.0001 seconds\n",
            "Epoch 2, Loss: 2.8151439112059924, Time: 11.7855 seconds\n",
            "Epoch 3, Loss: 2.6001506192343578, Time: 12.1927 seconds\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "_LinAlgError",
          "evalue": "linalg.svd: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: 128).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-750284873.py\u001b[0m in \u001b[0;36mpolar_retraction\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# torch >= 2.1: exact polar factorization, numerically stable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.linalg' has no attribute 'polar'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2212064498.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epochs: {args.epochs} --- LR: {args.lr}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"--- WD: {args.wd}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"adam\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     model, epoch_losses, epoch_times = train(\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0minitial_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2212064498.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, initial_lr, update, wd)\u001b[0m\n\u001b[1;32m     81\u001b[0m                       \u001b[0;31m# 2) Nouveau pas L-BFGS (note le opt=..., et p.data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                       \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                           \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                     \u001b[0;31m# Cas stateless\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2967691343.py\u001b[0m in \u001b[0;36mmanifold_muon_general\u001b[0;34m(W, G, eta, alpha, steps, tol, opt)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;31m# Take one quasi-Newton step on the manifold (retraction included inside .step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# ManifoldLBFGS.step handles internal transposition and returns W in the original orientation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mnew_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# The new_W returned by opt.step is already in the original orientation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1595747558.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, W, G)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m# Ambient move + retraction back to the Stiefel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0mW_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mW_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# Cache objects needed to form (s_k, y_k) once G_new is available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1595747558.py\u001b[0m in \u001b[0;36m_retract\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# _retract ensures that W stays a Stiefel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_retract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolar_retraction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_polar_impl\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-750284873.py\u001b[0m in \u001b[0;36mpolar_retraction\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Fallback: X = U Σ V^T  => polar(X) = U V^T\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mVt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_LinAlgError\u001b[0m: linalg.svd: The algorithm failed to converge because the input matrix is ill-conditioned or has too many repeated singular values (error code: 128)."
          ]
        }
      ],
      "source": [
        "\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"gpu\")\n",
        "\n",
        "# Redefine the problematic functions to be more robust\n",
        "@torch.no_grad()\n",
        "def polar_retraction(X):\n",
        "    # Try torch.linalg.polar for PyTorch 2.1+\n",
        "    try:\n",
        "        U, _ = torch.linalg.polar(X)\n",
        "        return U\n",
        "    except AttributeError: # Catches 'module 'torch.linalg' has no attribute 'polar''\n",
        "        pass # Fallback to SVD\n",
        "    except Exception as e:\n",
        "        # Catch other exceptions from torch.linalg.polar, if any\n",
        "        print(f\"Warning: torch.linalg.polar failed with {type(e).__name__}: {e}. Falling back to SVD.\")\n",
        "        pass # Fallback to SVD\n",
        "\n",
        "    # Fallback to SVD for older PyTorch or if polar decomposition fails\n",
        "    try:\n",
        "        # Use torch.svd (older, sometimes more numerically robust) or torch.linalg.svd\n",
        "        # Adding a small epsilon to avoid potential LinalgError on ill-conditioned matrices\n",
        "        # 'some=False' in torch.svd is equivalent to 'full_matrices=False' in torch.linalg.svd\n",
        "        U, S, V = torch.svd(X + 1e-7 * torch.randn_like(X), some=False)\n",
        "        return U @ V.T\n",
        "    except Exception as e:\n",
        "        print(f\"Error during SVD fallback: {type(e).__name__}: {e}. Trying again with more robust linalg.svd.\")\n",
        "        try:\n",
        "            # If torch.svd also fails, try torch.linalg.svd with a perturbation\n",
        "            U, _, Vt = torch.linalg.svd(X + 1e-7 * torch.randn_like(X), full_matrices=False)\n",
        "            return U @ Vt\n",
        "        except Exception as svd_e:\n",
        "            raise RuntimeError(f\"Both SVD methods failed to converge: {svd_e}\") from svd_e\n",
        "\n",
        "@torch.no_grad()\n",
        "def msign(W):\n",
        "    try:\n",
        "        U, _, V = torch.linalg.svd(W, full_matrices=False)\n",
        "        return U @ V.T\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: msign's torch.linalg.svd failed with {type(e).__name__}: {e}. Trying again with perturbation.\")\n",
        "        U, _, V = torch.linalg.svd(W + 1e-7 * torch.randn_like(W), full_matrices=False)\n",
        "        return U @ V.T\n",
        "\n",
        "@torch.no_grad()\n",
        "def project_to_stiefel(W):\n",
        "    try:\n",
        "        U, _, V = torch.linalg.svd(W, full_matrices=False)\n",
        "        return U @ V.T\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: project_to_stiefel's torch.linalg.svd failed with {type(e).__name__}: {e}. Trying again with perturbation.\")\n",
        "        U, _, V = torch.linalg.svd(W + 1e-7 * torch.randn_like(W), full_matrices=False)\n",
        "        return U @ V.T\n",
        "\n",
        "\n",
        "# Ensure ManifoldLBFGS uses these re-defined functions or is also re-defined if needed.\n",
        "# For now, we assume these global redefinitions will be picked up by ManifoldLBFGS if it references them globally.\n",
        "# If ManifoldLBFGS is a class from the %run notebook, you might need to redefine it here as well.\n",
        "# As a temporary measure, let's also redefine ManifoldLBFGS to ensure it picks up the new polar_retraction and msign.\n",
        "\n",
        "# --- ManifoldLBFGS redefinition START ---\n",
        "# This is a placeholder. You need to copy the actual ManifoldLBFGS class definition\n",
        "# from Optimizers_project_182.ipynb here and ensure it uses the globally redefined\n",
        "# polar_retraction and msign. Below is a generic structure assuming it relies on global functions.\n",
        "\n",
        "# To ensure this fix works, you *must* copy the full ManifoldLBFGS class definition\n",
        "# from '/content/drive/MyDrive/Colab_Notebooks/EECS182_project/Optimizers_project_182.ipynb'\n",
        "# into this cell and make sure its _retract method uses the globally defined polar_retraction/msign.\n",
        "# If its _retract method is hard-coded to a local version, this global override won't work.\n",
        "\n",
        "# For simplicity and assuming the original ManifoldLBFGS relies on global scope for polar_retraction and msign,\n",
        "# we'll use a simplified re-run of the notebook to get the class itself, but the globally defined functions above\n",
        "# should take precedence if referenced correctly.\n",
        "\n",
        "%run \"/content/drive/MyDrive/Colab_Notebooks/EECS182_project/hyperspherical_descent.ipynb\"\n",
        "%run \"/content/drive/MyDrive/Colab_Notebooks/EECS182_project/Optimizers_project_182.ipynb\"\n",
        "\n",
        "# --- ManifoldLBFGS redefinition END ---\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
        "\n",
        "\n",
        "OPTS = {}\n",
        "\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 128, bias=False)\n",
        "        self.fc2 = nn.Linear(128, 64, bias=False)\n",
        "        self.fc3 = nn.Linear(64, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 32 * 32 * 3)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def train(epochs, initial_lr, update, wd):\n",
        "    model = MLP().cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if update == AdamW:\n",
        "        optimizer = AdamW(model.parameters(), lr=initial_lr, weight_decay=wd)\n",
        "    else:\n",
        "        assert update in [manifold_muon, hyperspherical_descent, manifold_muon_general]\n",
        "        optimizer = None\n",
        "        if update == manifold_muon_general:\n",
        "          opts = {p: ManifoldLBFGS(eta=initial_lr, history=10, eps_curv=1e-12, use_polar_impl=True) for p in model.parameters()}\n",
        "\n",
        "    steps = epochs * len(train_loader)\n",
        "    step = 0\n",
        "\n",
        "    if optimizer is None:\n",
        "      for p in model.parameters():\n",
        "          if update == manifold_muon_general:\n",
        "              # Use the robust project_to_stiefel defined globally\n",
        "              p.data = project_to_stiefel(p.data) # This line was removed as it's typically done once per param and `update` handles retraction.\n",
        "              # Initial retraction is handled by ManifoldLBFGS itself now, or by `update` func for stateless optimizers.\n",
        "              # The original code had a separate `project_to_stiefel` call here; I'm re-adding it for consistency if it was intended.\n",
        "              # However, usually the optimizers handle the initial projection if needed.\n",
        "              # For ManifoldLBFGS, the `step` method (via `_retract`) will do the retraction.\n",
        "\n",
        "              # If initial projection is truly needed outside the optimizer step, ensure `project_to_stiefel` is used.\n",
        "              # Given the original error with `manifold_muon_general`, we need to ensure the initial `p.data` is on the manifold.\n",
        "              p.data = project_to_stiefel(p.data)\n",
        "          else:\n",
        "              p.data = update(p.data, torch.zeros_like(p.data), eta=0.0)\n",
        "\n",
        "\n",
        "    epoch_losses = []\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            lr = initial_lr * (1 - step / steps)\n",
        "            with torch.no_grad():\n",
        "                if optimizer is None:\n",
        "                    if update == manifold_muon_general:\n",
        "                      # 1) Finaliser la paire (s,y) précédente avec le gradient courant\n",
        "                      for p in model.parameters():\n",
        "                        if getattr(opts[p], \"last\", None) is not None:\n",
        "                          opts[p].update(p.grad)\n",
        "\n",
        "                      # 2) Nouveau pas L-BFGS (note le opt=..., et p.data)\n",
        "                      for p in model.parameters():\n",
        "                          p.data = update(p.data, p.grad, eta=lr, opt=opts[p])\n",
        "                else:\n",
        "                    # Cas stateless\n",
        "                    for p in model.parameters():\n",
        "                        p.data = update(p.data, p.grad, eta=lr)\n",
        "\n",
        "            step += 1\n",
        "            running_loss += loss.item()\n",
        "            if (i+1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_time = end_time - start_time\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        epoch_times.append(epoch_time)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}, Time: {epoch_time:.4f} seconds\")\n",
        "    return model, epoch_losses, epoch_times\n",
        "\n",
        "\n",
        "def eval(model):\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        accs = []\n",
        "        for dataloader in [test_loader, train_loader]:\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in dataloader:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            accs.append(100 * correct / total)\n",
        "\n",
        "    print(f\"Accuracy of the network on the {len(test_loader.dataset)} test images: {accs[0]} %\")\n",
        "    print(f\"Accuracy of the network on the {len(train_loader.dataset)} train images: {accs[1]} %\")\n",
        "    return accs\n",
        "\n",
        "def weight_stats(model):\n",
        "    singular_values = []\n",
        "    norms = []\n",
        "    for p in model.parameters():\n",
        "        try:\n",
        "            # Use linalg.svdvals for singular values if available and stable\n",
        "            s = torch.linalg.svdvals(p)\n",
        "        except RuntimeError:\n",
        "            # Fallback to full svd then take singular values if svdvals fails\n",
        "            _, s, _ = torch.linalg.svd(p, full_matrices=False)\n",
        "        except AttributeError: # For older PyTorch where svdvals might not exist\n",
        "            _, s, _ = torch.svd(p, some=False)\n",
        "\n",
        "        singular_values.append(s)\n",
        "        norms.append(p.norm())\n",
        "    return singular_values, norms\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train a model on CIFAR-10.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of epochs to train for.\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.1, help=\"Initial learning rate.\")\n",
        "    parser.add_argument(\"--update\", type=str, default=\"manifold_muon_general\", choices=[\"manifold_muon\", \"hyperspherical_descent\", \"adam\",\"manifold_muon_general\"], help=\"Update rule to use.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed for the random number generator.\")\n",
        "    parser.add_argument(\"--wd\", type=float, default=0.0, help=\"Weight decay for AdamW.\")\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    # determinism flags\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    update_rules = {\n",
        "        \"manifold_muon\": manifold_muon,\n",
        "        \"hyperspherical_descent\": hyperspherical_descent,\n",
        "        \"adam\": AdamW,\n",
        "        \"manifold_muon_general\": manifold_muon_general\n",
        "    }\n",
        "\n",
        "    update = update_rules[args.update]\n",
        "\n",
        "    print(f\"Training with: {args.update}\")\n",
        "    print(f\"Epochs: {args.epochs} --- LR: {args.lr}\", f\"--- WD: {args.wd}\" if args.update == \"adam\" else \"\")\n",
        "\n",
        "    model, epoch_losses, epoch_times = train(\n",
        "        epochs=args.epochs,\n",
        "        initial_lr=args.lr,\n",
        "        update=update,\n",
        "        wd=args.wd\n",
        "    )\n",
        "    test_acc, train_acc = eval(model)\n",
        "    singular_values, norms = weight_stats(model)\n",
        "\n",
        "    results = {\n",
        "        \"epochs\": args.epochs,\n",
        "        \"lr\": args.lr,\n",
        "        \"seed\": args.seed,\n",
        "        \"wd\": args.wd,\n",
        "        \"update\": args.update,\n",
        "        \"epoch_losses\": epoch_losses,\n",
        "        \"epoch_times\": epoch_times,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"singular_values\": singular_values,\n",
        "        \"norms\": norms\n",
        "    }\n",
        "\n",
        "    filename = f\"update-{args.update}-lr-{args.lr}-wd-{args.wd}-seed-{args.seed}.pkl\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    print(f\"Saving results to {os.path.join(\"results\", filename)}\")\n",
        "    with open(os.path.join(\"results\", filename), \"wb\") as f:\n",
        "        pickle.dump(results, f)\n",
        "    print(f\"Results saved to {os.path.join(\"results\", filename)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Test pour manifold_muon_général**"
      ],
      "metadata": {
        "id": "qid4FPCowwS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import math\n",
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# --- Missing definitions (from hyperspherical_descent.ipynb and Optimizers_project_182.ipynb) --- Start\n",
        "@torch.no_grad()\n",
        "def msign(W):\n",
        "    U, _, V = torch.linalg.svd(W, full_matrices=False)\n",
        "    return U @ V.T\n",
        "\n",
        "@torch.no_grad()\n",
        "def project_to_stiefel(W):\n",
        "    U, _, V = torch.linalg.svd(W, full_matrices=False)\n",
        "    return U @ V.T\n",
        "\n",
        "# Placeholder for ManifoldLBFGS if not globally available, otherwise ensure it's imported.\n",
        "# Assuming it's made available by the %run command from Optimizers_project_182.ipynb in vX_PDnXkgWkM.\n",
        "# If this cell is run independently, ManifoldLBFGS would need to be defined or explicitly imported.\n",
        "# For this fix, we assume it's available from the previously run notebooks.\n",
        "# If you encounter a NameError for ManifoldLBFGS, you might need to add its definition here\n",
        "# or ensure the %run command from the original cell is executed first.\n",
        "# For now, I'm assuming it's available.\n",
        "\n",
        "# --- Missing definitions --- End\n",
        "\n",
        "# -----------------------\n",
        "# Utils: device & seeds\n",
        "# -----------------------\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def set_determinism(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "@torch.no_grad()\n",
        "def manifold_muon(W, G, eta=0.1, alpha=0.01, steps=100, tol=1e-6):\n",
        "    # Ensure tall matrices\n",
        "    should_tranpose = W.shape[0] < W.shape[1]\n",
        "    if should_tranpose:\n",
        "        W = W.T; G = G.T\n",
        "\n",
        "    # Dual variable init\n",
        "    Lambda = -0.25 * (W.T @ G + G.T @ W)\n",
        "\n",
        "    # Dual ascent to find A ~ tangent sign direction\n",
        "    for step in range(steps):\n",
        "        A = msign(G + 2 * W @ Lambda)       # polar of (G + 2 Λ)\n",
        "        H = W.T @ A + A.T @ W               # tangency residual\n",
        "        if torch.norm(H) / math.sqrt(H.numel()) < tol:\n",
        "            break\n",
        "        Lambda -= alpha * (1 - step / steps) * H\n",
        "\n",
        "    # Primal step + retraction\n",
        "    new_W = W - eta * A\n",
        "    new_W = msign(new_W)\n",
        "    return new_W.T if should_tranpose else new_W\n",
        "\n",
        "@torch.no_grad()\n",
        "def manifold_muon_general(W, G, eta=0.1, *, opt):\n",
        "    \"\"\"Wrapper that delegates the step to a persistent ManifoldLBFGS instance.\"\"\"\n",
        "    opt.eta = eta\n",
        "    return opt.step(W, G)\n",
        "\n",
        "# -----------------------\n",
        "# CIFAR-10 & Model\n",
        "# -----------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124),\n",
        "                         (0.24703233, 0.24348505, 0.26158768))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(dataset=test_dataset,  batch_size=1024, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(32 * 32 * 3, 128, bias=False)\n",
        "        self.fc2 = nn.Linear(128, 64, bias=False)\n",
        "        self.fc3 = nn.Linear(64, 10,  bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# -----------------------\n",
        "# Train / Eval\n",
        "# -----------------------\n",
        "def train(epochs, initial_lr, update, wd):\n",
        "    model = MLP().to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = None\n",
        "\n",
        "    # Per-parameter L-BFGS states (only for manifold_muon_general)\n",
        "    opts = None\n",
        "    if update == manifold_muon_general:\n",
        "        opts = {p: ManifoldLBFGS(eta=initial_lr, history=10, use_polar_impl=True) for p in model.parameters()}\n",
        "\n",
        "    steps = epochs * len(train_loader)\n",
        "    step = 0\n",
        "\n",
        "    # One-shot projection to the manifold\n",
        "    with torch.no_grad():\n",
        "        for p in model.parameters():\n",
        "            if update == manifold_muon_general:\n",
        "                p.data = project_to_stiefel(p.data)\n",
        "            elif update == manifold_muon or update == hyperspherical_descent:\n",
        "                p.data = update(p.data, torch.zeros_like(p.data), eta=0.0)  # retract via msign inside\n",
        "\n",
        "    epoch_losses, epoch_times = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            # Forward\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Schedule LR (simple linear decay)\n",
        "            lr = float(initial_lr) * (1.0 - step / max(1, steps))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if optimizer is None:\n",
        "                    if update == manifold_muon_general:\n",
        "                        # 1) Finalize previous L-BFGS step with current grads (lazy curvature pair)\n",
        "                        for p in model.parameters():\n",
        "                            if opts[p].last is not None:\n",
        "                                opts[p].update(p.grad)\n",
        "\n",
        "                        # 2) Take a NEW L-BFGS manifold step\n",
        "                        for p in model.parameters():\n",
        "                            p.data = manifold_muon_general(p.data, p.grad, eta=lr, opt=opts[p])\n",
        "\n",
        "                    else:  # manifold_muon or hyperspherical_descent (stateless)\n",
        "                        for p in model.parameters():\n",
        "                            p.data = update(p.data, p.grad, eta=lr)\n",
        "\n",
        "                else:\n",
        "                    for g in optimizer.param_groups:\n",
        "                        g[\"lr\"] = lr\n",
        "                    optimizer.step()\n",
        "\n",
        "            # These lines were moved outside the if/else for optimizer to ensure they are always executed\n",
        "            step += 1\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}] Step [{i+1}/{len(train_loader)}] Loss: {loss.item():.4f}\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_time = end_time - start_time\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        epoch_times.append(epoch_time)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Time: {epoch_time:.2f}s\")\n",
        "\n",
        "    return model, epoch_losses, epoch_times\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_model(model):\n",
        "    model.eval()\n",
        "    accs = []\n",
        "    for dataloader in [test_loader, train_loader]:\n",
        "        correct, total = 0, 0\n",
        "        for images, labels in dataloader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        accs.append(100.0 * correct / total)\n",
        "    print(f\"Accuracy on test set:  {accs[0]:.2f}%\")\n",
        "    print(f\"Accuracy on train set: {accs[1]:.2f}%\")\n",
        "    return accs\n",
        "\n",
        "def weight_stats(model):\n",
        "    sv_list, norms = [], []\n",
        "    for p in model.parameters():\n",
        "        try:\n",
        "            s = torch.linalg.svdvals(p)\n",
        "        except RuntimeError:\n",
        "            # fallback: full svd then take singular values\n",
        "            _, s, _ = torch.linalg.svd(p, full_matrices=False)\n",
        "        sv_list.append(s)\n",
        "        norms.append(p.norm())\n",
        "    return sv_list, norms\n",
        "\n",
        "# -----------------------\n",
        "# Main\n",
        "# -----------------------\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train a model on CIFAR-10.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of epochs.\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.1, help=\"Initial learning rate.\")\n",
        "    parser.add_argument(\"--update\", type=str, default=\"manifold_muon_general\",\n",
        "                        choices=[\"manifold_muon_general\", \"manifold_muon\", \"adam\", \"hyperspherical_descent\"],\n",
        "                        help=\"Update rule to use.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed.\")\n",
        "    parser.add_argument(\"--wd\", type=float, default=0.0, help=\"Weight decay for AdamW.\")\n",
        "    # In notebooks, parse empty to keep defaults:\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    set_determinism(args.seed)\n",
        "\n",
        "    update_rules = {\n",
        "        \"manifold_muon_general\": manifold_muon_general,\n",
        "        \"manifold_muon\": manifold_muon,\n",
        "        \"adam\": AdamW,\n",
        "        \"hyperspherical_descent\": globals().get('hyperspherical_descent', None) # Safely get if globally available\n",
        "    }\n",
        "\n",
        "    # Ensure hyperspherical_descent is loaded or handled if it's a choice but not defined locally.\n",
        "    if update_rules[\"hyperspherical_descent\"] is None and args.update == \"hyperspherical_descent\":\n",
        "        raise NameError(\"hyperspherical_descent function is not defined. Ensure it's sourced from a %run notebook or defined locally.\")\n",
        "\n",
        "    update = update_rules[args.update]\n",
        "\n",
        "    print(f\"Training with: {args.update}\")\n",
        "    print(f\"Epochs: {args.epochs} — LR: {args.lr}\" + (f\" — WD: {args.wd}\" if args.update == \"adam\" else \"\"))\n",
        "\n",
        "    model, epoch_losses, epoch_times = train(\n",
        "        epochs=args.epochs,\n",
        "        initial_lr=args.lr,\n",
        "        update=update,\n",
        "        wd=args.wd\n",
        "    )\n",
        "\n",
        "    test_acc, train_acc = eval_model(model)\n",
        "    singular_values, norms = weight_stats(model)\n",
        "\n",
        "    results = {\n",
        "        \"epochs\": args.epochs,\n",
        "        \"lr\": args.lr,\n",
        "        \"seed\": args.seed,\n",
        "        \"wd\": args.wd,\n",
        "        \"update\": args.update,\n",
        "        \"epoch_losses\": epoch_losses,\n",
        "        \"epoch_times\": epoch_times,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"singular_values\": [s.cpu() for s in singular_values],\n",
        "        \"norms\": [n.item() for n in norms],\n",
        "    }\n",
        "\n",
        "    filename = f\"update-{args.update}-lr-{args.lr}-wd-{args.wd}-seed-{args.seed}.pkl\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    save_path = os.path.join(\"results\", filename)\n",
        "    print(f\"Saving results to {save_path}\")\n",
        "    with open(save_path, \"wb\") as f:\n",
        "        pickle.dump(results, f)\n",
        "    print(f\"Results saved to {save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "PJqOh8Eqwu9W",
        "outputId": "34a5c775-ac23-481e-8311-386b6026498b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with: manifold_muon_general\n",
            "Epochs: 5 — LR: 0.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'project_to_stiefel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1546523779.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epochs: {args.epochs} — LR: {args.lr}\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\" — WD: {args.wd}\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"adam\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     model, epoch_losses, epoch_times = train(\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0minitial_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1546523779.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, initial_lr, update, wd)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmanifold_muon_general\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;31m# Ensure project_to_stiefel is accessible (e.g., from an earlier %run notebook)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_to_stiefel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmanifold_muon\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mhyperspherical_descent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# retract via msign inside\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'project_to_stiefel' is not defined"
          ]
        }
      ]
    }
  ]
}