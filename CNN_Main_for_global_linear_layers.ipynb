{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6d+LZiA+g3bFYRRh1rhEe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fantiflex/MuOn-optimizer/blob/main/CNN_Main_for_global_linear_layers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"gpu\")\n",
        "\n",
        "%run \"/content/drive/MyDrive/Colab_Notebooks/EECS182_project/hyperspherical_descent.ipynb\"\n",
        "%run \"/content/drive/MyDrive/Colab_Notebooks/EECS182_project/LGFBS_global.ipynb\"\n",
        "# after this, the functions defined inside those notebooks are available in the current notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX_PDnXkgWkM",
        "outputId": "b5e00c11-0bde-493a-e026-dc71067cc3a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "em6Mx4-5gGZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91dae524-ba98-47b2-ee9d-004f4159ba25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with: manifold_muon_general\n",
            "Epochs: 5 --- LR: 0.5 \n",
            "Nb de paramètres sur la manifold : 4\n",
            "Epoch 1, Loss: 2.2883199769623426, Time: 20.6788 seconds\n",
            "Epoch 2, Loss: 2.215462621377439, Time: 20.0864 seconds\n",
            "Epoch 3, Loss: 1.9384604862758092, Time: 19.6220 seconds\n",
            "Epoch 4, Loss: 1.6169386712872251, Time: 18.9903 seconds\n",
            "Epoch 5, Loss: 1.427728957059432, Time: 19.4774 seconds\n",
            "Accuracy of the network on the 10000 test images: 51.45 %\n",
            "Accuracy of the network on the 50000 train images: 51.898 %\n",
            "Saving results to results/update-manifold_muon_general-lr-0.5-wd-0.0-seed-42.pkl\n",
            "Results saved to results/update-manifold_muon_general-lr-0.5-wd-0.0-seed-42.pkl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.49139968, 0.48215827, 0.44653124), (0.24703233, 0.24348505, 0.26158768))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=1024, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1024, shuffle=False)\n",
        "\n",
        "\n",
        "OPTS = {}\n",
        "\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # 3 x 32 x 32  -> 128 x 8 x 8\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),              # 64 x 16 x 16\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),              # 128 x 8 x 8\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128 * 8 * 8, 256, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 256, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 10, bias=False),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x arrive déjà en (B, 3, 32, 32) grâce au DataLoader\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  # flatten\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def is_linear_weight(p):\n",
        "    # on considère comme \"Stiefel\" uniquement les matrices 2D (weights des Linear)\n",
        "    return p.dim() == 2\n",
        "\n",
        "\n",
        "def train(epochs, initial_lr, update, wd):\n",
        "    model = CNN().cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    if update == AdamW:\n",
        "        optimizer = AdamW(model.parameters(), lr=initial_lr, weight_decay=wd)\n",
        "        opts = None\n",
        "    else:\n",
        "        assert update in [manifold_muon, hyperspherical_descent, manifold_muon_general]\n",
        "        optimizer = None\n",
        "        opts = None\n",
        "        if update == manifold_muon_general:\n",
        "            # état L-BFGS seulement pour les matrices 2D (Linear)\n",
        "            opts = {\n",
        "                p: ManifoldLBFGS(eta=initial_lr, history=10, eps_curv=1e-12)\n",
        "                for p in model.parameters()\n",
        "                if is_linear_weight(p)\n",
        "            }\n",
        "            print(\"Nb de paramètres sur la manifold :\", len(opts))\n",
        "\n",
        "        if update == manifold_muon:\n",
        "            nb_manifold = sum(1 for p in model.parameters() if is_linear_weight(p))\n",
        "            print(\"Nb de paramètres sur la manifold (muon classique) :\", nb_manifold)\n",
        "\n",
        "    steps = epochs * len(train_loader)\n",
        "    step = 0\n",
        "\n",
        "    # --- Projection initiale sur la manifold : seulement pour les Linear ---\n",
        "    if optimizer is None:\n",
        "        for p in model.parameters():\n",
        "            if is_linear_weight(p) and update in [manifold_muon, manifold_muon_general]:\n",
        "                if update == manifold_muon_general:\n",
        "                    p.data = manifold_muon_general(\n",
        "                        p.data, torch.zeros_like(p.data), eta=0.0, opt=opts[p]\n",
        "                    )\n",
        "                else:  # muon classique\n",
        "                    p.data = manifold_muon(\n",
        "                        p.data, torch.zeros_like(p.data), eta=0.0\n",
        "                    )\n",
        "            # sinon: on laisse convs & biais tranquilles\n",
        "\n",
        "    epoch_losses = []\n",
        "    epoch_times = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        running_loss = 0.0\n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            model.zero_grad()\n",
        "            loss.backward()\n",
        "            lr = initial_lr * (1 - step / steps)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if optimizer is None:\n",
        "                    # --- Cas muon / hyperspherical_descent ---\n",
        "                    if update == manifold_muon_general:\n",
        "                        # 1) update des paires (s,y) pour les Linear\n",
        "                        for p in model.parameters():\n",
        "                            if is_linear_weight(p) and getattr(opts.get(p, None), \"last\", None) is not None:\n",
        "                                opts[p].update(p.grad)\n",
        "\n",
        "                        # 2) step : muon général sur Linear, step euclidien pour le reste\n",
        "                        for p in model.parameters():\n",
        "                            if is_linear_weight(p):\n",
        "                                p.data = manifold_muon_general(\n",
        "                                    p.data, p.grad, eta=lr, opt=opts[p]\n",
        "                                )\n",
        "                            else:\n",
        "                                p.data -= lr * p.grad\n",
        "\n",
        "                    elif update == manifold_muon:\n",
        "                        # muon classique seulement sur les Linear\n",
        "                        for p in model.parameters():\n",
        "                            if is_linear_weight(p):\n",
        "                                p.data = manifold_muon(p.data, p.grad, eta=lr)\n",
        "                            else:\n",
        "                                p.data -= lr * p.grad\n",
        "\n",
        "                    elif update == hyperspherical_descent:\n",
        "                        # à toi de décider : tout ou seulement certains paramètres\n",
        "                        for p in model.parameters():\n",
        "                            p.data = hyperspherical_descent(p.data, p.grad, eta=lr)\n",
        "\n",
        "                else:\n",
        "                    # Cas AdamW\n",
        "                    optimizer.step()\n",
        "\n",
        "            step += 1\n",
        "            running_loss += loss.item()\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "        end_time = time.time()\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        epoch_time = end_time - start_time\n",
        "        epoch_losses.append(epoch_loss)\n",
        "        epoch_times.append(epoch_time)\n",
        "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss}, Time: {epoch_time:.4f} seconds\")\n",
        "\n",
        "    return model, epoch_losses, epoch_times\n",
        "\n",
        "\n",
        "def eval(model):\n",
        "    # Test the model\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        accs = []\n",
        "        for dataloader in [test_loader, train_loader]:\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for images, labels in dataloader:\n",
        "                images = images.cuda()\n",
        "                labels = labels.cuda()\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            accs.append(100 * correct / total)\n",
        "\n",
        "    print(f\"Accuracy of the network on the {len(test_loader.dataset)} test images: {accs[0]} %\")\n",
        "    print(f\"Accuracy of the network on the {len(train_loader.dataset)} train images: {accs[1]} %\")\n",
        "    return accs\n",
        "\n",
        "def weight_stats(model):\n",
        "    singular_values = []\n",
        "    norms = []\n",
        "    for p in model.parameters():\n",
        "        u,s,v = torch.svd(p)\n",
        "        singular_values.append(s)\n",
        "        norms.append(p.norm())\n",
        "    return singular_values, norms\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser(description=\"Train a model on CIFAR-10.\")\n",
        "    parser.add_argument(\"--epochs\", type=int, default=5, help=\"Number of epochs to train for.\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.5, help=\"Initial learning rate.\")\n",
        "    parser.add_argument(\"--update\", type=str, default=\"manifold_muon_general\", choices=[\"manifold_muon\", \"hyperspherical_descent\", \"adam\",\"manifold_muon_general\"], help=\"Update rule to use.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Seed for the random number generator.\")\n",
        "    parser.add_argument(\"--wd\", type=float, default=0.0, help=\"Weight decay for AdamW.\")\n",
        "    args = parser.parse_args([])\n",
        "\n",
        "    # determinism flags\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.cuda.manual_seed_all(args.seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    update_rules = {\n",
        "        \"manifold_muon\": manifold_muon,\n",
        "        \"hyperspherical_descent\": hyperspherical_descent,\n",
        "        \"adam\": AdamW,\n",
        "        \"manifold_muon_general\": manifold_muon_general\n",
        "    }\n",
        "\n",
        "    update = update_rules[args.update]\n",
        "\n",
        "    print(f\"Training with: {args.update}\")\n",
        "    print(f\"Epochs: {args.epochs} --- LR: {args.lr}\", f\"--- WD: {args.wd}\" if args.update == \"adam\" else \"\")\n",
        "\n",
        "    model, epoch_losses, epoch_times = train(\n",
        "        epochs=args.epochs,\n",
        "        initial_lr=args.lr,\n",
        "        update=update,\n",
        "        wd=args.wd\n",
        "    )\n",
        "    test_acc, train_acc = eval(model)\n",
        "    singular_values, norms = weight_stats(model)\n",
        "\n",
        "    results = {\n",
        "        \"epochs\": args.epochs,\n",
        "        \"lr\": args.lr,\n",
        "        \"seed\": args.seed,\n",
        "        \"wd\": args.wd,\n",
        "        \"update\": args.update,\n",
        "        \"epoch_losses\": epoch_losses,\n",
        "        \"epoch_times\": epoch_times,\n",
        "        \"test_acc\": test_acc,\n",
        "        \"train_acc\": train_acc,\n",
        "        \"singular_values\": singular_values,\n",
        "        \"norms\": norms\n",
        "    }\n",
        "\n",
        "    filename = f\"update-{args.update}-lr-{args.lr}-wd-{args.wd}-seed-{args.seed}.pkl\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    print(f\"Saving results to {os.path.join(\"results\", filename)}\")\n",
        "    with open(os.path.join(\"results\", filename), \"wb\") as f:\n",
        "        pickle.dump(results, f)\n",
        "    print(f\"Results saved to {os.path.join(\"results\", filename)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efef7867"
      },
      "source": [
        "First, uninstall the current PyTorch version. This command will prompt you for confirmation, so make sure to type `y` and press Enter when asked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "997783d4",
        "outputId": "93c65324-6097-4af0-926a-f459aff66f7c"
      },
      "source": [
        "!pip uninstall torch torchvision torchaudio -y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.8.0+cu126\n",
            "Uninstalling torch-2.8.0+cu126:\n",
            "  Successfully uninstalled torch-2.8.0+cu126\n",
            "Found existing installation: torchvision 0.23.0+cu126\n",
            "Uninstalling torchvision-0.23.0+cu126:\n",
            "  Successfully uninstalled torchvision-0.23.0+cu126\n",
            "Found existing installation: torchaudio 2.8.0+cu126\n",
            "Uninstalling torchaudio-2.8.0+cu126:\n",
            "  Successfully uninstalled torchaudio-2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60eb6a2f"
      },
      "source": [
        "Next, install the desired PyTorch version. You can find the installation command for specific versions and CUDA compatibility on the official PyTorch website (`pytorch.org/get-started/locally/`).\n",
        "\n",
        "For example, to install the latest stable version with CUDA 11.8 (common in Colab), you might use something like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "762b7306",
        "outputId": "1c9c073b-48f8-4228-a17b-4cf701b803f7"
      },
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m828.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (905.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.2/905.2 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp312-cp312-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.4.0\n",
            "    Uninstalling triton-3.4.0:\n",
            "      Successfully uninstalled triton-3.4.0\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "torchvision",
                  "triton"
                ]
              },
              "id": "74c53ccc3bd444ee946d96403bc54164"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d58f48e2"
      },
      "source": [
        "After running these commands, you should restart the runtime (`Runtime > Restart runtime` from the menu) for the changes to take effect. Then, you can verify the installed version:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ebf5f9",
        "outputId": "f79787b6-c4e1-40b9-fc39-ef419b4bc0a5"
      },
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c573f376"
      },
      "source": [
        "Keep in mind that Colab environments are updated periodically, so a specific PyTorch version might become the default over time."
      ]
    }
  ]
}